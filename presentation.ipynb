{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inverse Artificial Neural Network (ANN) ODE Solver\n",
    "\n",
    "### Summer 2019 CX4240 - Project\n",
    "\n",
    "Dr. Mahdi Roozbahani\n",
    "\n",
    "#### [**Project Proposal Document**](./proposal/proposal.pdf)\n",
    "\n",
    "### Group:\n",
    "\n",
    "   - Gabriel Sabenca Gusmao **[GSG]**\n",
    "   - Nicole Yuge Hu **[NYH]**\n",
    "   - Zhenzi Yu  **[ZY]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1. Introduction:\n",
    "\n",
    "**a. Background: Chemical Reactions**\n",
    "\n",
    "$$A\\overset{k_1}{\\to} B\\overset{k_2}{\\to} C\\notag $$     \n",
    "\n",
    "Set of coupled ordinary differential equations (ODEs):\n",
    "\\begin{align}\n",
    "\\frac{dC_A}{dt}& = -k_{1} C_A \\\\\n",
    "\\frac{dC_B}{dt}& = k_1 C_A-k_2 C_B\\\\\n",
    "\\frac{dC_C}{dt}& = k_2 C_B \\\\\n",
    "\\end{align}\n",
    "\n",
    "**b. Goal: solve Ordinary Differential Equations (ODEs).**\n",
    "  - Traditional solution: discrete, iterative, problem dependent.\n",
    "  - Neural Network: flexible, continuous, parallelizable.    \n",
    "  \n",
    "**c. Validation: Can a neural network learn (interpolate) the ODE numerical solution?**\n",
    "\n",
    "<img src=\"./invANNODE/basis/intro.gif\">\n",
    "\n",
    "**d. Forward problem: get solution only using k-value**\n",
    "\n",
    "**f. Inverse Problem: get k-value using data**\n",
    "\n",
    "   - Model Selection and Parameter Fitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2. Model: Inverse ANN solution for ODE's for model selection and parameter regression.\n",
    "\n",
    " \n",
    "#### 2.1 Forward Problem \n",
    "\n",
    "   The forward problem consists of solving the following minimization problem, where $\\theta_{i}$ is a general array of parameters of $i$. In the forward mode, the model and its parameters, $\\theta_{model}$ are known, and state variables $x(t)$ are solved for. $\\gamma$ generalizes the chosen norm.\n",
    "   \n",
    "   $$\\underset{\\theta_{ANN}}{\\min}\\|\\dot{x}(\\theta_{ANN} | t)-f(x|\\theta_{model}) \\|_{\\gamma}$$\n",
    "   \n",
    "   for $f(x)=\\dot{x}$ and $\\dot{x}$ can be estimated as of backprogapation (chain rule) through the ANN, in which case we shall use automatic differentiation.\n",
    "   \n",
    "#### 2.2 Inverse Problem\n",
    "\n",
    "   Now state variables $x(t)$ are given and neither the model nor its parameters are known.\n",
    "   \n",
    "   $$\\underset{\\theta_{ANN},\\theta_{model}}{\\min}\\alpha\\|\\tilde{x}(t)-x(\\theta_{ANN} | t) \\|_{\\gamma}+(1-\\alpha)\\|\\dot{x}(\\theta_{ANN} | t)-f(x|\\theta_{model}) \\|_{\\gamma}+\\mathcal{R}(\\theta_{model})$$\n",
    "   \n",
    "   $\\alpha$ can be atrbitrarily defined or evaluated in order to minimize the cost function. If $\\alpha\\to1$, the ANN accurately maps data from $t$ to $x$ but is not necessarily attached to the model; the opposite being true as $\\alpha\\to0$. $\\mathcal{R}(\\theta_{model})$ is a regularization term on model parameters to avoid derailing during the iterative minimization process.\n",
    "   \n",
    "   Regularization might not be necessary if cross-validation is used in the learning process.   \n",
    "      \n",
    "   <p></p>\n",
    "<div align=\"center\">\n",
    "<b> Project Diagram</b>\n",
    "</div>\n",
    "  <img src=\"./imgs/fig3.png\" alt=\"overview\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3. Result & Discussion\n",
    "\n",
    "\n",
    "#### 3.1 Forward problem\n",
    "NN ODE forward solution and derivatives of model and ANN\n",
    "\n",
    "</div>\n",
    "  <img src=\"./invANNODE/basis/fwd.gif\" alt=\"overview\"> \n",
    "\n",
    "- ANN can readily be applied to solve forward ODE problems by interpolating between data. \n",
    "- ANN method has gradient converge to model after about 80 iterations. \n",
    "\n",
    "#### 3.2 Inverse problem\n",
    "NN ODE inverse solution and derivatives of model and ANN\n",
    "\n",
    "</div>\n",
    "    <img src=\"./invANNODE/basis/non_noise.gif\" alt=\"overview\"> \n",
    "\n",
    "Model parameter comparison at epochs = 200\n",
    "\n",
    "| True $k_{1}$ |Regressed $ k_{1}$ | True $k_{2}$ |Regressed $ k_{2}$ |\n",
    "|--------------|-------------------|--------------|-------------------|\n",
    "| 1.00000      | 0.98567           |  1.00000     | 0.98181           |\n",
    "\n",
    "\n",
    "- NN can also be applied to solve inverse ODE problems and extract model parameters. \n",
    "\n",
    "#### 3.3 Noisy data\n",
    "Added noise to data generated to model actual experimental errors.\n",
    "\n",
    "ANN ODE noisy data solution and derivatives of model and NN\n",
    "\n",
    "</div>\n",
    "    <img src=\"./invANNODE/basis/noisy.gif\" alt=\"overview\"> \n",
    "\n",
    "- NN model adapt well to data with significant noises.\n",
    "- One hidden layer and small number of nodes were used in ANN to avoid over-fitting.\n",
    "\n",
    "\n",
    "### Discussion\n",
    "\n",
    "- NN has the advantages of being continuous, differentiable, closed-form solution in ANN meaning that it requires less memory, but at the same time, it requires more time to train on the dataset. \n",
    "- NN can solve kinetics ODE in forward and inverse fashion. \n",
    "- NN also works with noisy dataset that mimics actual experiment measurements. \n",
    "\n",
    "### Future work:\n",
    "\n",
    "- Compatibility of ANN with stiff problems\n",
    "- Optimize discretization of domain like ivp_solver. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Similar projects\n",
    "\n",
    "[**Neural Ordinary Differential Equations**](https://arxiv.org/abs/1806.07366v4) (https://arxiv.org/abs/1806.07366)   \n",
    "\n",
    "&ensp; [2] Chen, R. T. Q., Rubanova, Y., Bettencourt, J. & Duvenaud, D. Neural Ordinary Differential Equations. (2018).\n",
    "\n",
    "   - Extensive study on the forward problem automatic differentiation capability.\n",
    "   - Does not explore inverse problems.\n",
    "\n",
    "[**Hidden Physics Models: Machine Learning of Nonlinear Partial Differential Equations**](https://arxiv.org/abs/1708.00588) (https://arxiv.org/abs/1708.00588)   \n",
    "\n",
    "&ensp; [3] Raissi, M. & Karniadakis, G. E. Hidden Physics Models: Machine Learning of Nonlinear Partial Differential Equations. (2017).\n",
    "   \n",
    "   - Non-linear ordinary and partial differential equation identification.\n",
    "   - Uses Gaussian Processes as scaffold for learning.\n",
    "   - Assumes model is known apriori.\n",
    "   \n",
    "[**Solving coupled ODEs with a neural network and autograd**](http://kitchingroup.cheme.cmu.edu/blog/category/ode/ ) (Kitchin's Group)\n",
    "\n",
    "   - Solves the forward ODE for kinetic systems using ANN.\n",
    "   - Show example of automatic differentiation with *autograd*.\n",
    "   - Kitchin's work on the forward problem will be used as starting point for the inverse problem.\n",
    "   \n",
    "[**Multistep Neural Networks for Data-driven Discovery of Nonlinear Dynamical Systems**](https://arxiv.org/abs/1801.01236) (https://arxiv.org/abs/1801.01236)   \n",
    "\n",
    "\n",
    "&ensp; [4] Raissi, M., Perdikaris, P. & Karniadakis, G. E. Multistep Neural Networks for Data-driven Discovery of Nonlinear Dynamical Systems. (2018).\n",
    "\n",
    "   - Solves multiple kinds of PDE's using ANN.\n",
    "   - Explore different time-stepping formulas.\n",
    "   - Discusses whole of noise and regularization\n",
    "   - Does not entail parameter fitting.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Library Dependencies\n",
    "\n",
    "\n",
    "- Automatic differentiation: [**JAX**](https://github.com/google/jax) (https://github.com/google/jax)\n",
    "- Array-operation: [**numpy**](https://www.numpy.org/)\n",
    "- Solution for initial-value problems, IVPs (forward problem): [**dasslc2py**](https://github.com/asanet/dasslc2py) (https://github.com/asanet/dasslc2py) or [**scipy**](https://www.scipy.org/)\n",
    "- Non-linear minimization (optimization): [**scipy**](https://www.scipy.org/) ... for now.\n",
    "- Artificial Neural Networks (ANN): [**TensorFlow**](https://www.tensorflow.org/) (https://www.tensorflow.org/)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
