{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Wed Jun 19 17:45:50 2019\n",
    "\n",
    "@author: ggusmao3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "import pickle as pk\n",
    "import jax.numpy as np\n",
    "from numpy.random import choice\n",
    "from jax import grad, jit, vmap, jacobian, jacfwd, jacrev\n",
    "from jax import random\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax.experimental import optimizers\n",
    "from jax.config import config\n",
    "from jax.tree_util import tree_map\n",
    "config.update(\"jax_debug_nans\", True)\n",
    "config.update('jax_enable_x64', True)\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"### Hyperparameters\n",
    "Let's get a few bookkeeping items out of the way.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_params(m, key, scale=1e-2):\n",
    "    #w_key, b_key = random.split(key)\n",
    "    #print(tuple(scale * random.normal(key, (m, 1))))\n",
    "    return (scale * random.normal(key, (m,)))#, scale * random.normal(b_key, (n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key, scale):\n",
    "    keys = random.split(key, len(sizes))\n",
    "    return [random_layer_params(m, n, k, scale) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(size, key, scale):\n",
    " key = random.split(key,2)[-1] \n",
    " return random_params(size, key, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def transfer_fun(x):\n",
    "    #return np.maximum(0, x)\n",
    "    #return np.sin(x)\n",
    "    #return np.log(np.cosh(x))\n",
    "    return 2./(1.+np.exp(-2.*x))-1.\n",
    "    # return \n",
    "    #return np.nan_to_num(x / (1.0 + np.exp(-x)))\n",
    "    #return x / (1.0 + np.exp(-x))\n",
    "    #return np.exp(-x**2)\n",
    "    #return 2./(1.+np.exp(-2.*x))-1.    \n",
    "    #return 0.5*np.tanh(x) + 0.5*x / (1.0 + np.exp(-x))\n",
    "    #return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def state(params, t):\n",
    "    # per-example stateions\n",
    "    activations = t\n",
    "    for w, b in params[:-1]:\n",
    "        outputs = np.dot(w, activations) + b\n",
    "        activations = transfer_fun(outputs)\n",
    "    \n",
    "    final_w, final_b = params[-1]\n",
    "    y = (np.dot(final_w, activations) + final_b)\n",
    "    #y = y / y.sum()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a batched version of the `state` function\n",
    "batched_state = vmap(state, in_axes=(None,0))#, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def diff_state(params,t,pos):\n",
    "    i = np.arange(len(t))\n",
    "    #return (jacobian(batched_state,argnums=1)(params,t)[i,:,i,0])\n",
    "    return (jacfwd(lambda t : batched_state(params,t))(t)[i,:,i,pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward NN-based PDE\n",
    "To solve forward problems, in general random or error-based sampling does not change.\n",
    "\n",
    "#### MAYBE WE CAN USE RELU WITH A DENSE MATRIX AND A VANISHING NONLINEAR OPERATOR TO FIND BEST COLOCATTION MATRICES@"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@jit\n",
    "def source(source_params,x):\n",
    "        #print('source x: {}'.format(x))\n",
    "        #x = np.abs(np.nan_to_num(x))\n",
    "        k = source_params\n",
    "        return np.array([\n",
    "                        [-k[0]*x[0]*x[6]+k[1]*x[3]],\n",
    "                        [-k[2]*x[1]*x[6]+k[3]*x[4]],\n",
    "                        [ k[6]*x[5]-k[7]*x[2]*x[6]],\n",
    "                        [ k[0]*x[0]*x[6]-k[1]*x[3]-k[4]*x[3]*x[4]+k[5]*x[5]*x[6]],\n",
    "                        [ k[2]*x[1]*x[6]-k[3]*x[4]-k[4]*x[3]*x[4]+k[5]*x[5]*x[6]],\n",
    "                        [ k[4]*x[3]*x[4]-k[5]*x[5]*x[6]-k[6]*x[5]+k[7]*x[2]*x[6]],\n",
    "                        [-k[0]*x[0]*x[6]+k[1]*x[3]-k[2]*x[1]*x[6]+k[3]*x[4]\\\n",
    "                        +k[4]*x[3]*x[4]-k[5]*x[5]*x[6]+k[6]*x[5]-k[7]*x[2]*x[6]]\n",
    "                        ])\n",
    "        \n",
    "        \"\"\"\n",
    "        return np.array([[-mp[0]*(x[0])],0\n",
    "                         [mp[0]*(x[0]) - mp[1]*((x[1]))],\n",
    "                         [mp[1]*((x[1]))]])\n",
    "        \"\"\"\n",
    "batched = vmap(source,in_axes=(None,0))#, in_axes=(None, 0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "source_params0 = np.array([1.,2.,1.5,1.3,1.2*100.,2.*100.,3.,1.5])\n",
    "bc0=np.array([.5,.2,.3,0.,0.,0.,1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def source(source_params,x):\n",
    "    #print('source x: {}'.format(x))\n",
    "    x = np.abs(np.nan_to_num(x))\n",
    "    k = np.abs(source_params)\n",
    "    return -k[0]*x\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.array([[-mp[0]*(x[0])],0\n",
    "                     [mp[0]*(x[0]) - mp[1]*((x[1]))],\n",
    "                     [mp[1]*((x[1]))]])\n",
    "    \"\"\"\n",
    "batched = vmap(source,in_axes=(None,0,None))#, in_axes=(None, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_params = np.array([3.])\n",
    "bc0=np.array([1.])\n",
    "bcf=np.array([0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [1, 8, 1]\n",
    "layer_sizes2 = [1, 8, 1]\n",
    "source_size = len(source_params)\n",
    "nn_scale = .01\n",
    "D = 0.01\n",
    "num_epochs = 100\n",
    "num_eras = 100"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.save('nn_p_1o',nn_p,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "key = random.PRNGKey(0)\n",
    "try:\n",
    "        nn_p = [tuple(np.array(_,np.float64) for _ in __) for __ in np.load('nn_p_1o.npy',allow_pickle=True).tolist()]\n",
    "except:\n",
    "        nn_p = init_network_params(layer_sizes, key, nn_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "nn_p = init_network_params(layer_sizes, key, nn_scale)\n",
    "nn_p2 = init_network_params(layer_sizes2, key, nn_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 20\n",
    "batch_size = 20#n_points-1\n",
    "tf = 1\n",
    "#src = np.logspace(0,np.log10(tf+1),n_points).reshape(-1,1)-1.\n",
    "src = np.linspace(-tf,tf,n_points).reshape(-1,1)\n",
    "data = src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def get_errors_f(nn_p, nn_p2, batch):\n",
    "    s = batch\n",
    "    x_s = batched_state(nn_p, s)\n",
    "    dx_s_ = batched_state(nn_p2, s)\n",
    "    dx_s = diff_state(nn_p, s, 0)\n",
    "    d2x_s_ = diff_state(nn_p2, s, 0)\n",
    "    err = 1e5*(((d2x_s_-s**5))**2).sum(axis=1)#((s+1)**4/(x_s+1)**2)*\n",
    "    err_coupling = (((dx_s_-dx_s))**2).sum(axis=1)#+((dx_s[0]-1)**2).sum(axis=1)#(((((s+1)**2)**2/(x_s+1)**2)*(d2x_s-1))**2).sum(axis=1)#+((dx_s[0]-1)**2).sum(axis=1)((s+1)**2/(x_s+1)**2)*\n",
    "    err_bc =     (((batched_state(nn_p, np.array([[0.]])))**2).sum()+((batched_state(nn_p, np.array([[1.]])))**2).sum(axis=1))\n",
    "    return err, err_coupling, err_bc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def loss_f(nn_p, nn_p2, batch):\n",
    "    __ = get_errors_f(nn_p, nn_p2, batch)     \n",
    "    return sum([_.mean()    for _ in __])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_init, opt_update, get_params = optimizers.adam(1e-3, b1=0.9, b2=0.9999, eps=1e-100)\n",
    "opt_init2, opt_update2, get_params2 = optimizers.adam(1e-3, b1=0.9, b2=0.9999, eps=1e-100)\n",
    "    \n",
    "@jit\n",
    "def step(i, opt_state, opt_state2, batch):\n",
    "    nn_p = get_params(opt_state)    \n",
    "    nn_p2 = get_params(opt_state2)    \n",
    "    grads = grad(loss_f,argnums=0)(nn_p, nn_p2, batch)\n",
    "    grads2 = grad(loss_f,argnums=1)(nn_p, nn_p2, batch)\n",
    "    opt = opt_update(i, grads, opt_state)\n",
    "    opt2 = opt_update2(i, grads2, opt_state2)\n",
    "    return [opt, opt2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itercount    = itertools.count()        \n",
    "\n",
    "opt_state = opt_init(nn_p)\n",
    "opt_state2 = opt_init(nn_p2)\n",
    "\n",
    "for j in range(num_eras):\n",
    "    sel = random.shuffle(random.PRNGKey(j),np.arange(n_points))     \n",
    "    #err, _, _ = get_errors_f(nn_p_space, nn_p_space_sub, data)\n",
    "    #err_dist = err#+err_L.mean(axbis=1)#+err_B.mean(axis=1)#+err.mean(axis=1)\n",
    "    #err_dist = err_dist/err_dist.sum()        \n",
    "    #sel = np.concatenate((np.array([0]),choice(np.arange(len(data)),batch_size,False,err_dist.flatten())))\n",
    "    batch = data[sel[:batch_size],:]\n",
    "    #batch = data\n",
    "    loss_it_data0 = np.inf\n",
    "    loss_it_sample0 = np.inf\n",
    "    for i in range(int(num_epochs)):\n",
    "        #err, err_L, err_B, err_BC = get_errors_f(nn_p,data)\n",
    "        print('Iteration: {:4d}, Loss Batch: {:.7e}, Loss Data: {:.7e}'.format(i,loss_it_sample0,loss_it_data0))\n",
    "        opt_state, opt_state2 = step(next(itercount), opt_state, opt_state2, batch)\n",
    "        nn_p = get_params(opt_state)\n",
    "        nn_p2 = get_params(opt_state2)\n",
    "        loss_it_sample = loss_f(nn_p, nn_p2, batch)\n",
    "        loss_it_data = loss_f(nn_p, nn_p2, data)\n",
    "        loss_it_sample0 = loss_it_sample\n",
    "        loss_it_data0 = loss_it_data    \n",
    "        clear_output(wait=True)\n",
    "    #\n",
    "nn_p = get_params(opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_errors_f(nn_p, nn_p2, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10*2./3],dpi=100)\n",
    "tf = 1\n",
    "src2 = np.linspace(-tf,tf,1000).reshape(-1,1)\n",
    "plt.plot(src2, batched_state(nn_p,src2),'-',label='NN')\n",
    "plt.plot(src, src**7/30.-1/30.*src,'o',label='function')\n",
    "#plt.gca().set_yscale('log')\n",
    "#plt.plot(src, diff_state(nn_p,src,0),'.',label='f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python3",
   "formats": "ipynb,py:light",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
