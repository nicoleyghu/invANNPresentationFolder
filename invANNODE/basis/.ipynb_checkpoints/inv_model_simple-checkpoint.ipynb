{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Wed Jun 19 17:45:50 2019\n",
    "\n",
    "@author: ggusmao3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap, jacobian, jacfwd, jacrev\n",
    "from jax import random\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax.experimental import optimizers\n",
    "from jax.config import config\n",
    "config.update(\"jax_debug_nans\", True)\n",
    "config.update('jax_enable_x64', True)\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"### Hyperparameters\n",
    "Let's get a few bookkeeping items out of the way.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_model_params(m, key, scale=1e-2):\n",
    "    #w_key, b_key = random.split(key)\n",
    "    #print(tuple(scale * random.normal(key, (m, 1))))\n",
    "    return np.abs(scale * random.normal(key, (m,)))#, scale * random.normal(b_key, (n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key, scale):\n",
    "    keys = random.split(key, len(sizes))\n",
    "    return [random_layer_params(m, n, k, scale) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_params(size, key, scale):\n",
    " key = random.split(key,2)[-1] \n",
    " return random_model_params(size, key, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def transfer_fun(x):\n",
    "    #return np.maximum(0, x)\n",
    "    #return np.nan_to_num(x / (1.0 + np.exp(-x)))\n",
    "    return x / (1.0 + np.exp(-x))\n",
    "    #return np.tanh(x)\n",
    "    #return 0.5*np.tanh(x) + 0.5*x / (1.0 + np.exp(-x))\n",
    "    #return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def state(params, t):\n",
    "    # per-example stateions\n",
    "    activations = t\n",
    "    for w, b in params[:-1]:\n",
    "        outputs = np.dot(w, activations) + b\n",
    "        activations = transfer_fun(outputs)\n",
    "    \n",
    "    final_w, final_b = params[-1]\n",
    "    y = np.dot(final_w, activations) + final_b\n",
    "    #y = y / y.sum()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a batched version of the `state` function\n",
    "batched_state = vmap(state, in_axes=(None,0))#, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def accuracy(params, t, targets):\n",
    "   target_class = np.argm25ax(targets, axi25s=1)\n",
    "   stateed_class = np.argmax(batched_state(params, t), axis=1)\n",
    "   return np.mean(stateed_class == target_class)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def accuracy(params, t, target_x):\n",
    "   pred_x = batched_state(params,t)\n",
    "   y    = pred_x-np.mean(pred_x) \n",
    "   y_ = target_x-np.mean(target_x)\n",
    "   ##print(np.shape(y),np.shape(y_))\n",
    "   return np.mean(np.dot(y.T,y_)**2/(np.dot(y.T,y)*np.dot(y_.T,y_)))*np.exp(-np.mean((pred_x-target_x)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def diff_state(params,t):\n",
    "        i = np.arange(len(t))\n",
    "        return (jacobian(batched_state,argnums=1)(params,t)[i,:,i,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def model(model_params,x):\n",
    "        #print('model x: {}'.format(x))\n",
    "        #x = np.abs(np.nan_to_num(x))\n",
    "        mp = model_params\n",
    "        return np.array([[-mp[0]*(x[0])],\n",
    "                                         [mp[0]*(x[0]) - mp[1]*((x[1]))],\n",
    "                                         [mp[1]*((x[1]))]])\n",
    "        \n",
    "        \"\"\"\n",
    "        return np.array([[-mp[0]*(x[0]**2)+mp[3]*x[3]**2+mp[3]*x[2]**2],\n",
    "                                         [mp[0]*(x[0]**2) - mp[1]*((x[1])**1.5)],\n",
    "                                         [mp[1]*((x[1])**1.5)-mp[3]*x[2]**2],\n",
    "                                         [mp[2]*(x[1]*x[0])-mp[3]*x[3]**2]])\n",
    "        \"\"\"\n",
    "\n",
    "batched_model = vmap(model, in_axes=(None,0))#, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_params = np.array([1.,1.,0.5,1.])\n",
    "model_params0 = np.array([1.,1.])\n",
    "#bc=np.array([1.0,0.,0.,0.])\n",
    "bc0=np.array([1.0,0.,0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tmax = 10\n",
    "t = np.logspace(0,np.log10(tmax+1),60).reshape((-1,1))-1.\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "def ode(t,C):\n",
    "        return model(model_params0,C).flatten()\n",
    "\n",
    "n_points = 50\n",
    "\n",
    "t_eval = np.logspace(0,np.log10(tmax+1),n_points)-1.\n",
    "#t_eval = np.linspace(0,20,n_points)\n",
    "sol = solve_ivp(ode, (0, tmax), bc0, t_eval = t_eval)\n",
    "\n",
    "t = sol.t\n",
    "x0 = sol.y.T\n",
    "x = x0#+random.normal(random.PRNGKey(0),sol.y.T.shape)*0.025\n",
    "plt.plot(sol.t, x0)\n",
    "plt.plot(sol.t, x,'.-',ms=5,lw=0.5)\n",
    "#plt.legend(['A', 'B', 'C', 'D'])\n",
    "plt.legend(['A', 'B', 'C'])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('C');\n",
    "\n",
    "t = t.reshape([-1,1])\n",
    "\n",
    "batch = (x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def loss(nn_params, model_params, batch):\n",
    "    x, t = batch\n",
    "    pred_x = (batched_state(nn_params, t))\n",
    "    err_model = ((diff_state(nn_params,t).reshape([-1,1])-batched_model(model_params_,pred_x)[:,:,0].reshape([-1,1]))**2)\n",
    "    err_data    =    ((x-pred_x)**2)#((x-pred_x)**2).sum(axis=0).mean()\n",
    "    #print('pred[0]: {}'.format(pred_x[0]))\n",
    "    #print('sum pred[0] - bc: {}'.format(sum((pred_x[0]-bc)**2)))\n",
    "    return ((1.+err_data.var()))*err_data.mean()#+err_model.mean()#+np.sum([-np.nan_to_num(np.log(-i)) for i in model_params])#((pred_x[0]-bc)**2).mean()#+((pred_x.sum(axis=1)-1.)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#@jit\n",
    "#def update(params, t):\n",
    "#    grads = jacobian(loss,argnums=0)(params, t)\n",
    "#    #grads = [tuple(i.sum(axis=0) for i in j) for j in grads]\n",
    "#    return [(w - step_size * dw, b - step_size * db)\n",
    "#                    for (w, b), (dw, db) in zip(params, grads)]\n",
    "# Use optimizers to set optimizer initialization and update functions\n",
    "\n",
    "layer_sizes = [1, 15, 3]\n",
    "model_size = 2\n",
    "param_scale = .01\n",
    "model_scale = 1.5\n",
    "num_epochs = 1000\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "nn_params = init_network_params(layer_sizes, key, param_scale)\n",
    "model_params = init_model_params(model_size, key, model_scale)\n",
    "\n",
    "opt_init_nn, opt_update_nn, get_params_nn = optimizers.adam(1e-3, b1=0.9, b2=0.999, eps=1e-90)#, b1=0.1, b2=0.999, eps=1e-10)\n",
    "opt_init_model, opt_update_model, get_params_model = optimizers.adam(1e-2, b1=0.9, b2=0.999, eps=1e-90)\n",
    "\n",
    "for i in range(10000):\n",
    "        \n",
    "        @jit\n",
    "        def step_nn(i, opt_state, batch):\n",
    "            nn_params = get_params_nn(opt_state_nn)\n",
    "            model_params = get_params_model(opt_state_model)\n",
    "            grads_nn = grad(loss,argnums=0)(nn_params, model_params, batch)    \n",
    "            return opt_update_nn(i, grads_nn, opt_state_nn)\n",
    "\n",
    "        #\"\"\"\n",
    "        @jit\n",
    "        def step_model(i, opt_state, batch):\n",
    "            nn_params = get_params_nn(opt_state_nn)\n",
    "            model_params = get_params_model(opt_state_model)\n",
    "            model_params_ = model_params\n",
    "            grads_model = grad(loss,argnums=1)(nn_params, model_params, batch)    \n",
    "            return opt_update_model(i, grads_model, opt_state_model)\n",
    "        #\"\"\"\n",
    "\n",
    "        opt_state_nn = opt_init_nn(nn_params)\n",
    "        opt_state_model = opt_init_model(model_params)\n",
    "        model_params_ = model_params\n",
    "        for i in range(int(num_epochs)):\n",
    "            nn_params = get_params_nn(opt_state_nn)\n",
    "            model_params = get_params_model(opt_state_model)\n",
    "            print('Iteration: {}, Loss: {}'.format(i,loss(nn_params, model_params, batch)))\n",
    "            opt_state_nn        = step_nn(i, opt_state_nn, batch)\n",
    "            #opt_state_model = step_model(i, opt_state_model, batch)\n",
    "            clear_output(wait=True)\n",
    "            #print('--.--')    \n",
    "        print('Iteration: {}, Loss: {}'.format(i,loss(nn_params, model_params, batch)))\n",
    "nn_params = get_params_nn(opt_state_nn)\n",
    "params_model = get_params_model(opt_state_model)\n",
    "#print(np.shape(opt_state.packed_state[3][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#print((batched_state(nn_params,model_params,x)).sum(axis=1))\n",
    "#print((sol.y).sum(axis=0))\n",
    "plt.plot(t, x)\n",
    "plt.plot(t.flatten(),batched_state(nn_params, t),'-o',lw=0.2,ms=2)\n",
    "plt.legend(['A', 'B', 'C', 'D'])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('C');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python3",
   "formats": "ipynb,py:light",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
