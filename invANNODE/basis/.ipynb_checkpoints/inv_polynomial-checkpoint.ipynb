{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Wed Jun 19 17:45:50 2019\n",
    "\n",
    "@author: ggusmao3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "import pickle as pk\n",
    "import jax.numpy as np\n",
    "from numpy.random import choice\n",
    "from jax import grad, jit, vmap, jacobian, jacfwd, jacrev\n",
    "from jax import random\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax.experimental import optimizers\n",
    "from jax.config import config\n",
    "from jax.tree_util import tree_map\n",
    "config.update(\"jax_debug_nans\", True)\n",
    "config.update('jax_enable_x64', True)\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"### Hyperparameters\n",
    "Let's get a few bookkeeping items out of the way.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_M_params(m, key, scale=1e-2):\n",
    "    #w_key, b_key = random.split(key)\n",
    "    #print(tuple(scale * random.normal(key, (m, 1))))\n",
    "    return (scale * random.normal(key, (m,)))#, scale * random.normal(b_key, (n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key, scale):\n",
    "    keys = random.split(key, len(sizes))\n",
    "    return [random_layer_params(m, n, k, scale) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_M_params(size, key, scale):\n",
    " key = random.split(key,2)[-1] \n",
    " return random_M_params(size, key, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def transfer_fun(x):\n",
    "    #return np.maximum(0, x)\n",
    "    #return np.nan_to_num(x / (1.0 + np.exp(-x)))\n",
    "    #return x / (1.0 + np.exp(-x))\n",
    "    #return np.tanh(x)\n",
    "    #return np.exp(-x**2)    \n",
    "    #return x\n",
    "    return np.nan_to_num(np.true_divide(2.,(1.+np.exp(-2.*x)))-1)\n",
    "    #return 0.5*np.tanh(x) + 0.5*x / (1.0 + np.exp(-x))\n",
    "    #return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def state(params, t):\n",
    "    # per-example stateions\n",
    "    activations = t\n",
    "    for w, b in params[:-1]:\n",
    "        outputs = np.dot(w, activations) + b\n",
    "        activations = transfer_fun(outputs)\n",
    "    \n",
    "    final_w, final_b = params[-1]\n",
    "    y = (np.dot(final_w, activations) + final_b)\n",
    "    #y = y / y.sum()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a batched version of the `state` function\n",
    "batched_state = vmap(state, in_axes=(None,0))#, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def diff_state(params,t):\n",
    "        i = np.arange(len(t))\n",
    "        #return (jacobian(batched_state,argnums=1)(params,t)[i,:,i,0])\n",
    "        return np.nan_to_num(jacfwd(lambda t : batched_state(params,t))(t)[i,:,i,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes_poly = [1, 200, 2000, 1]\n",
    "#model_params = model_params0.copy()\n",
    "p_total = 100.\n",
    "alpha = 0.01\n",
    "nn_scale = .1\n",
    "model_scale = 1.\n",
    "num_epochs = 100\n",
    "num_eras = 10000\n",
    "latent_variables = 4\n",
    "\n",
    "batch_size = 99#n_points-1\n",
    "tf = 20.\n",
    "\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "nn_params_poly = init_network_params(layer_sizes_poly, key, nn_scale)\n",
    "\n",
    "order = 3\n",
    "t = np.linspace(-1,1,10).reshape(-1,1)\n",
    "\n",
    "@jit\n",
    "def fun(x):\n",
    "        return x**2\n",
    "\n",
    "#model_params = init_M_params(model_size, key, model_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def get_errors(nn_params_poly,batch):\n",
    "    x, t = batch\n",
    "    pred_x = (batched_state(nn_params_poly, t))\n",
    "    #err_M = (diff_state(batched_state(nn_params_chain, pred_x)))**2\n",
    "    err_data    =    (x-pred_x)#+(diff_state(nn_params_int,t)-pred_x[:,:-latent_variables])**2\n",
    "    return err_data#, 0.*err_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def loss(nn_params_poly, batch):\n",
    "    #print('pred[0]: {}'.format(pred_x[0]))\n",
    "    #print('sum pred[0] - bc: {}'.format(sum((pred_x[0]-bc)**2))) \n",
    "    return sum([_.var() for _ in get_errors(nn_params_poly,batch)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%prun\n",
    "opt_init_poly, opt_update_poly, get_params_poly = optimizers.adam(1e-3, b1=0.9, b2=0.9999, eps=1e-5)#, b1=0.1, b2=0.999, eps=1e-10)\n",
    "    \n",
    "@jit\n",
    "def step(i, opt_state_poly, batch):\n",
    "    nn_params_poly = get_params_poly(opt_state_poly)\n",
    "    grads_poly = grad(loss,argnums=0)(nn_params_poly, batch)\n",
    "    return [opt_update_poly(i, grads_poly, opt_state_poly)]\n",
    " \n",
    "itercount    = itertools.count()        \n",
    "\n",
    "opt_state_poly = opt_init_poly(nn_params_poly)\n",
    "\n",
    "for j in range(num_eras):\n",
    "        #err, _ = get_errors(nn_params_poly,nn_params_chain,t)\n",
    "        #err_dist = err.mean(axis=1)#+err_L.mean(axbis=1)#+err_B.mean(axis=1)#+err_M.mean(axis=1)\n",
    "        #err_dist = err_dist/err_dist.sum()     \n",
    "        #sel = np.concatenate((np.array([0]),choice(np.arange(len(data[0])),batch_size,False,err_dist.flatten())))\n",
    "        batch = tuple([fun(t),t])\n",
    "        #batch = data\n",
    "        for i in range(int(num_epochs)):\n",
    "            it = next(itercount)\n",
    "            opt_state_poly, = step(it, opt_state_poly, batch)\n",
    "            nn_params_poly = get_params_poly(opt_state_poly)\n",
    "            loss_it_sample = loss(nn_params_poly, batch)    \n",
    "            #loss_it_data = loss(nn_params_poly, nn_params_chain, data)\n",
    "            #print('Iteration: {:4d}, Loss Batch: {:.7e}, Loss ata: {:.7e}'.format(i,loss_it_sample,loss_it_data))\n",
    "            print('Iteration: {:4d}, Loss Batch: {:.7e}, '.format(i,loss_it_sample)) \n",
    "            clear_output(wait=True)\n",
    "            #print('--.--')                                \n",
    "nn_params_poly = get_params_nn(opt_state_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t,fun(t))\n",
    "plt.plot(t,batched_state(nn_params_poly,t),'.-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python3",
   "formats": "ipynb,py:light",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
