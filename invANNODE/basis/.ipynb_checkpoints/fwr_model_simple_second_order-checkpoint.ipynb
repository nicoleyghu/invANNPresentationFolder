{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Wed Jun 19 17:45:50 2019\n",
    "\n",
    "@author: ggusmao3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "import pickle as pk\n",
    "import jax.numpy as np\n",
    "from numpy.random import choice\n",
    "from jax import grad, jit, vmap, jacobian, jacfwd, jacrev\n",
    "from jax import random\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax.experimental import optimizers\n",
    "from jax.config import config\n",
    "from jax.tree_util import tree_map\n",
    "config.update(\"jax_debug_nans\", True)\n",
    "config.update('jax_enable_x64', True)\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"### Hyperparameters\n",
    "Let's get a few bookkeeping items out of the way.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_params(m, key, scale=1e-2):\n",
    "    #w_key, b_key = random.split(key)\n",
    "    #print(tuple(scale * random.normal(key, (m, 1))))\n",
    "    return (scale * random.normal(key, (m,)))#, scale * random.normal(b_key, (n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key, scale):\n",
    "    keys = random.split(key, len(sizes))\n",
    "    return [random_layer_params(m, n, k, scale) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(size, key, scale):\n",
    " key = random.split(key,2)[-1] \n",
    " return random_params(size, key, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def transfer_fun(x):\n",
    "    #return np.maximum(0, x)\n",
    "    #return np.sin(x)\n",
    "    #return np.log(np.cosh(x))\n",
    "    return 2./(1.+np.exp(-2.*x))-1.\n",
    "    #return np.nan_to_num(x / (1.0 + np.exp(-x)))\n",
    "    #return x / (1.0 + np.exp(-x))\n",
    "    #return np.exp(-x**2)\n",
    "    #return 2./(1.+np.exp(-2.*x))-1.    \n",
    "    #return 0.5*np.tanh(x) + 0.5*x / (1.0 + np.exp(-x))\n",
    "    #return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def transfer_fun2(x):\n",
    "    #return np.maximum(0, x)\n",
    "    #return np.sin(x)*np.cos(x)\n",
    "    #return np.nan_to_num(x / (1.0 + np.exp(-x)))\n",
    "    #return x / (1.0 + np.exp(-x))\n",
    "    #return np.exp(-x**2)\n",
    "    return 2./(1.+np.exp(-2.*x))-1.    \n",
    "    #return np.cos(x)    \n",
    "    #return 0.5*np.tanh(x) + 0.5*x / (1.0 + np.exp(-x))\n",
    "    #return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def state(params, t):\n",
    "    # per-example stateions\n",
    "    activations = t\n",
    "    for w, b in params[:-1]:\n",
    "        outputs = np.dot(w, activations) + b\n",
    "        activations = transfer_fun(outputs)\n",
    "    \n",
    "    final_w, final_b = params[-1]\n",
    "    y = (np.dot(final_w, activations) + final_b)\n",
    "    #y = y / y.sum()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a batched version of the `state` function\n",
    "batched_state = vmap(state, in_axes=(None,0))#, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def state2(params, t):\n",
    "    # per-example stateions\n",
    "    activations = t\n",
    "    for w, b in params[:-1]:\n",
    "        outputs = np.dot(w, activations) + b\n",
    "        activations = transfer_fun2(outputs)\n",
    "    \n",
    "    final_w, final_b = params[-1]\n",
    "    y = (np.dot(final_w, activations) + final_b)\n",
    "    #y = y / y.sum()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a batched version of the `state` function\n",
    "batched_state2 = vmap(state2, in_axes=(None,0))#, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def diff_state(params,t,pos):\n",
    "        i = np.arange(len(t))\n",
    "        #return (jacobian(batched_state,argnums=1)(params,t)[i,:,i,0])\n",
    "        return (jacfwd(lambda t : batched_state(params,t))(t)[i,:,i,pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def diff_state2(params,t,pos):\n",
    "        i = np.arange(len(t))\n",
    "        #return (jacobian(batched_state,argnums=1)(params,t)[i,:,i,0])\n",
    "        return (jacfwd(lambda t : batched_state2(params,t))(t)[i,:,i,pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward NN-based PDE\n",
    "To solve forward problems, in general random or error-based sampling does not change.\n",
    "\n",
    "#### MAYBE WE CAN USE RELU WITH A DENSE MATRIX AND A VANISHING NONLINEAR OPERATOR TO FIND BEST COLOCATTION MATRICES@"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@jit\n",
    "def source(source_params,x):\n",
    "        #print('source x: {}'.format(x))\n",
    "        #x = np.abs(np.nan_to_num(x))\n",
    "        k = source_params\n",
    "        return np.array([\n",
    "                                        [-k[0]*x[0]*x[6]+k[1]*x[3]],\n",
    "                                        [-k[2]*x[1]*x[6]+k[3]*x[4]],\n",
    "                                        [ k[6]*x[5]-k[7]*x[2]*x[6]],\n",
    "                                        [ k[0]*x[0]*x[6]-k[1]*x[3]-k[4]*x[3]*x[4]+k[5]*x[5]*x[6]],\n",
    "                                        [ k[2]*x[1]*x[6]-k[3]*x[4]-k[4]*x[3]*x[4]+k[5]*x[5]*x[6]],\n",
    "                                        [ k[4]*x[3]*x[4]-k[5]*x[5]*x[6]-k[6]*x[5]+k[7]*x[2]*x[6]],\n",
    "                                        [-k[0]*x[0]*x[6]+k[1]*x[3]-k[2]*x[1]*x[6]+k[3]*x[4]\\\n",
    "                                        +k[4]*x[3]*x[4]-k[5]*x[5]*x[6]+k[6]*x[5]-k[7]*x[2]*x[6]]\n",
    "                                        ])\n",
    "        \n",
    "        \"\"\"\n",
    "        return np.array([[-mp[0]*(x[0])],0\n",
    "                                           [mp[0]*(x[0]) - mp[1]*((x[1]))],\n",
    "                                           [mp[1]*((x[1]))]])\n",
    "        \"\"\"\n",
    "batched = vmap(source,in_axes=(None,0))#, in_axes=(None, 0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "source_params0 = np.array([1.,2.,1.5,1.3,1.2*100.,2.*100.,3.,1.5])\n",
    "bc0=np.array([.5,.2,.3,0.,0.,0.,1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def source(source_params,x):\n",
    "        #print('source x: {}'.format(x))\n",
    "        x = np.abs(np.nan_to_num(x))\n",
    "        k = np.abs(source_params)\n",
    "        return -k[0]*x\n",
    "        \n",
    "        \"\"\"\n",
    "        return np.array([[-mp[0]*(x[0])],0\n",
    "                                         [mp[0]*(x[0]) - mp[1]*((x[1]))],\n",
    "                                         [mp[1]*((x[1]))]])\n",
    "        \"\"\"\n",
    "batched = vmap(source,in_axes=(None,0,None))#, in_axes=(None, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_params = np.array([3.])\n",
    "bc0=np.array([0.])\n",
    "bcf=np.array([0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes_time = [1, 8, 1]\n",
    "layer_sizes_space = [1, 10, 1]\n",
    "layer_sizes_space_sub = [2, 10, 1]\n",
    "source_size = len(source_params)\n",
    "nn_scale = .01\n",
    "D = 0.01\n",
    "num_epochs = 5000\n",
    "num_eras = 100"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.save('nn_p',nn_p,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "key = random.PRNGKey(0)\n",
    "try:\n",
    "        nn_p = [tuple(np.array(_,np.float64) for _ in __) for __ in np.load('nn_p.npy',allow_pickle=True).tolist()]\n",
    "except:\n",
    "        nn_p = init_network_params(layer_sizes, key, nn_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "nn_p_space = init_network_params(layer_sizes_space, key, nn_scale)\n",
    "nn_p_space_sub = init_network_params(layer_sizes_space_sub, key, nn_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 5\n",
    "batch_size = 100#n_points-1\n",
    "tf = 1.\n",
    "#src = np.logspace(0,np.log10(tf+1),n_points).reshape(-1,1)-1.\n",
    "src = np.linspace(-1,1,n_points).reshape(-1,1)\n",
    "data = np.concatenate([_.reshape(-1,1) for _ in np.meshgrid(src,src)],axis=1)\n",
    "data = src.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def f1(x):\n",
    "        return 2./(1.+np.exp(-2.*x))-1.    \n",
    "\n",
    "@jit \n",
    "def f2(x):\n",
    "        return np.exp(-x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def get_errors_f(nn_p_space, nn_p_space_sub, batch):\n",
    "    s = batch\n",
    "    x_s = batched_state2(nn_p_space, s)\n",
    "    dx_s_ = diff_state2(nn_p_space, s, 0)\n",
    "    dx_s = batched_state(nn_p_space_sub, np.concatenate([s,dx_s_],axis=1))\n",
    "    d2x_s = diff_state(nn_p_space_sub, np.concatenate([s,dx_s_],axis=1),0)    \n",
    "    err = 100*((d2x_s-1)**2).sum(axis=1)#+((dx_s[0]-1)**2).sum(axis=1)#(((((s+1)**2)**2/(x_s+1)**2)*(d2x_s-1))**2).sum(axis=1)#+((dx_s[0]-1)**2).sum(axis=1)\n",
    "    err_coupling = (((dx_s-dx_s_))**2).sum(axis=1)    #((((s+1)**2/(x_s+1)**2)*(dx_s-dx_s_))**2).sum(axis=1)    \n",
    "    err_bc =     100*((x_s[0]-bc0)**2).sum() +\\\n",
    "                         100*((x_s[-1]-bcf)**2).sum()\n",
    "    return err, err_coupling, err_bc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def loss_f(nn_p_space, nn_p_space_sub, batch):\n",
    "    return sum([_.mean()    for _ in get_errors_f(nn_p_space, nn_p_space_sub, batch)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_init_space, opt_update_space, get_params_space = optimizers.adam(1e-3, b1=0.9, b2=0.9999, eps=1e-100)\n",
    "opt_init_space_sub, opt_update_space_sub, get_params_space_sub = optimizers.adam(1e-3, b1=0.9, b2=0.9999, eps=1e-100)\n",
    "    \n",
    "@jit\n",
    "def step(i, opt_state_space, nn_p_space_sub, batch):\n",
    "    nn_p_space = get_params_space(opt_state_space)    \n",
    "    nn_p_space_sub = get_params_space_sub(opt_state_space_sub)    \n",
    "    grads_space_sub = grad(loss_f,argnums=1)(nn_p_space, nn_p_space_sub, batch)\n",
    "    opt_sub = opt_update_space_sub(i, grads_space_sub, opt_state_space_sub)    \n",
    "    nn_p_space = get_params_space(opt_state_space)    \n",
    "    nn_p_space_sub = get_params_space_sub(opt_state_space_sub)    \n",
    "    grads_space = grad(loss_f,argnums=0)(nn_p_space, nn_p_space_sub, batch)\n",
    "    opt = opt_update_space(i, grads_space, opt_state_space)\n",
    "    return [opt,\n",
    "                 opt_sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itercount    = itertools.count()        \n",
    "\n",
    "opt_state_space = opt_init_space(nn_p_space)\n",
    "opt_state_space_sub = opt_init_space_sub(nn_p_space_sub)\n",
    "\n",
    "for j in range(num_eras):\n",
    "        #sel = ran0.1dom.shuffle(random.PRNGKey(j),np.arange(n_points))    \n",
    "        #err, _, _ = get_errors_f(nn_p_space, nn_p_space_sub, data)\n",
    "        #err_dist = err#+err_L.mean(axbis=1)#+err_B.mean(axis=1)#+err.mean(axis=1)\n",
    "        #err_dist = err_dist/err_dist.sum()     \n",
    "        #sel = np.concatenate((np.array([0]),choice(np.arange(len(data)),batch_size,False,err_dist.flatten())))\n",
    "        #batch = data[sel[:batch_size],:]\n",
    "        batch = data\n",
    "        loss_it_data0 = np.inf\n",
    "        loss_it_sample0 = np.inf\n",
    "        for i in range(int(num_epochs)):\n",
    "            #err, err_L, err_B, err_BC = get_errors_f(nn_p,data)\n",
    "            print('Iteration: {:4d}, Loss Batch: {:.7e}, Loss Data: {:.7e}'.format(i,loss_it_sample0,loss_it_data0))\n",
    "            it = next(itercount)\n",
    "            opt_state_space, opt_state_space_sub = step(next(itercount), opt_state_space, opt_state_space_sub, batch)\n",
    "            nn_p_space = get_params_space(opt_state_space)\n",
    "            nn_p_space_sub = get_params_space_sub(opt_state_space_sub)    \n",
    "            loss_it_sample = loss_f(nn_p_space, nn_p_space_sub, batch)\n",
    "            loss_it_data = loss_f(nn_p_space, nn_p_space_sub, data)\n",
    "            loss_it_sample0 = loss_it_sample\n",
    "            loss_it_data0 = loss_it_data    \n",
    "            clear_output(wait=True)\n",
    "        #\n",
    "nn_p = get_params(opt_state)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "grad(loss_f,argnums=1)(nn_p_space, nn_p_space_sub, batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_errors_f(nn_p_space, nn_p_space_sub, batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = lambda x, t : batched_state(nn_p_space,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10*2./3],dpi=100)\n",
    "plt.plot(src, batched_state(nn_p_space,src),'.-')\n",
    "plt.twinx()\n",
    "plt.plot(src, batched_state(nn_p_space_sub,src),'.-')\n",
    "plt.plot(src, diff_state(nn_p_space,src,0),'.-')\n",
    "plt.figure(figsize=[10,10*2./3],dpi=100)\n",
    "plt.plot(src, diff_state(nn_p_space_sub,src,0),'.-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python3",
   "formats": "ipynb,py:light",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
