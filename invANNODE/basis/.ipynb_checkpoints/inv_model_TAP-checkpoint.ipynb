{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Wed Jun 19 17:45:50 2019\n",
    "\n",
    "@author: ggusmao3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import\n",
    "import pickle as pk\n",
    "import jax.numpy as np\n",
    "from numpy.random import choice\n",
    "from jax import grad, jit, vmap, jacobian, jacfwd, jacrev\n",
    "from jax import random\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax.experimental import optimizers\n",
    "from jax.config import config\n",
    "from jax.tree_util import tree_map\n",
    "config.update(\"jax_debug_nans\", True)\n",
    "config.update('jax_enable_x64', True)\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import Javascript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"### Hyperparameters\n",
    "Let's get a few bookkeeping items out of the way.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_M_params(m, key, scale=1e-2):\n",
    "    #w_key, b_key = random.split(key)\n",
    "    #print(tuple(scale * random.normal(key, (m, 1))))\n",
    "    return (scale * random.normal(key, (m,)))#, scale * random.normal(b_key, (n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key, scale):\n",
    "    keys = random.split(key, len(sizes))\n",
    "    return [random_layer_params(m, n, k, scale) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_M_params(size, key, scale):\n",
    " key = random.split(key,2)[-1] \n",
    " return np.abs(random_M_params(size, key, scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def transfer_fun(x):\n",
    "    #return np.maximum(0, x)\n",
    "    #return np.nan_to_num(x / (1.0 + np.exp(-x)))\n",
    "    #return x / (1.0 + np.exp(-x))\n",
    "    #return np.tanh(x)\n",
    "    return np.nan_to_num(np.true_divide(2.,(1.+np.exp(-2.*x)))-1)\n",
    "    #return 0.5*np.tanh(x) + 0.5*x / (1.0 + np.exp(-x))\n",
    "    #return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def state(params, t):\n",
    "    # per-example stateions\n",
    "    activations = t\n",
    "    for w, b in params[:-1]:\n",
    "        outputs = np.dot(w, activations) + b\n",
    "        activations = transfer_fun(outputs)\n",
    "    \n",
    "    final_w, final_b = params[-1]\n",
    "    y = (np.dot(final_w, activations) + final_b)\n",
    "    #y = y / y.sum()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a batched version of the `state` function\n",
    "batched_state = vmap(state, in_axes=(None,0))#, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def diff_state(params,t):\n",
    "        i = np.arange(len(t))\n",
    "        #return (jacobian(batched_state,argnums=1)(params,t)[i,:,i,0])\n",
    "        return np.nan_to_num(jacfwd(lambda t : batched_state(params,t))(t)[i,:,i,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse TAP Problem\n",
    "\n",
    "To solve forward problems, in general random or error-based sampling does not change.\n",
    "\n",
    "#### MAYBE WE CAN USE RELU WITH A DENSE MATRIX AND A VANISHING NONLINEAR OPERATOR TO FIND BEST COLOCATTION MATRICES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.abspath('../../tap_data/lang_folder')\n",
    "labels_g = ['CO','CO2']\n",
    "labels_s = ['CO*','O*','*']\n",
    "species = dict(zip(range(len(labels_g+labels_s)),labels_g+labels_s))\n",
    "data_thin = []\n",
    "data_out = []\n",
    "df = pd.read_csv(path+'/input_file.csv',dtype= object,index_col= 0,header=None)\n",
    "total_time = np.float64(df.loc[df[1] == 'Pulse Duration'][2].values[0])\n",
    "total_steps = np.float64(df.loc[df[1] == 'Time Steps'][2].values[0])\n",
    "T = np.float64(df.loc[df[1] == 'Reactor Temperature'][2].values[0]) # K\n",
    "bc_s = np.float64(df.loc[df[1] == 'Initial Surface Composition'][2].values[0].split(',')) # nnmol/cm**3\n",
    "pulse_fracs = np.float64(df.loc[df[1] == 'Pulse Ratio'][2].values[0].split(','))[:-1] # nnmol/cm**3\n",
    "mw_g = np.float64(df.loc[df[1] == 'Mass List'][2].values[0].split(','))[:-1] # g/mol\n",
    "pulse =    np.float64(df.loc[df[1] == 'Reference Pulse Size'][2].values[0]) # cm\n",
    "length =    np.float64(df.loc[df[1] == 'Reactor Length'][2].values[0]) # cm\n",
    "radius =    np.float64(df.loc[df[1] == 'Reactor Radius'][2].values[0]) # cm\n",
    "alpha    =    np.float64(df.loc[df[1] == 'Catalyst Fraction'][2].values[0]) # fraction\n",
    "area = np.pi*radius**2\n",
    "cat_vol = length*area*alpha # cm**3\n",
    "tot_sites = cat_vol*bc_s.sum()*1e-9 # mol\n",
    "bc_s = bc_s/bc_s.sum() # frac\n",
    "for _ in labels_g:\n",
    "        try:\n",
    "                data_thin += [pd.read_csv(path+'/thin_data/'+_+'.csv').values]\n",
    "                data_out += [pd.read_csv(path+'/flux_data/'+_+'.csv').values]\n",
    "        except:\n",
    "                print('Error: {}.'.format(_))\n",
    "                raise(Exception)\n",
    "\n",
    "mw_s = np.array([mw_g[0], mw_g[1]-mw_g[0], 0])\n",
    "R = 8.31446261815324 # m**3.Pa/K.mol\n",
    "no_model_params = 3\n",
    "#data = [_*R*T*1e-8 for _ in data] # bar\n",
    "data_thin = [_*cat_vol for _ in data_thin] # nmol\n",
    "pulse_n = (pulse*mw_g*pulse_fracs).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "tspan = np.linspace(0,total_time,total_steps)\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(tspan,(np.array(data_thin).std(axis=2)/np.array(data_thin).mean(axis=2)).T)\n",
    "plt.gca().set_yscale('log')\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('relative deviation')\n",
    "plt.title('Relative Deviation Across Thin Zone')\n",
    "data_thin = pulse_n*np.array(data_thin).mean(axis=2).T\n",
    "data_out = pulse_n*np.array(data_out).mean(axis=2).T\n",
    "scale_thin = data_thin.max()\n",
    "scale_out = data_out.max()\n",
    "scale_t = tspan.max()\n",
    "data0 = [np.log(data_thin/scale_thin+1.)]+\\\n",
    "                [np.log(data_out/scale_out+1.)]+\\\n",
    "                [tspan.reshape(-1,1)/scale_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_thin_size = [1, 5, 5, len(species)]\n",
    "nn_outlet_size = [1, 5, 5, len(labels_g)]\n",
    "model_size = no_model_params\n",
    "#model_params = model_params0.copy()\n",
    "p_total = 100.\n",
    "alpha = 0.1\n",
    "nn_scale = .0001\n",
    "model_scale = 1e1\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "load_nn = False\n",
    "load_model = False\n",
    "\n",
    "loader = lambda _ : [[tuple(np.array(_,np.float64) for _ in __) \\\n",
    "                for __ in k] for k in np.load(_,allow_pickle=True).tolist()]\n",
    "\n",
    "if load_nn:\n",
    "        try:\n",
    "                nn_params_thin, nn_params_outlet = [[tuple(np.array(_,np.float64) for _ in __) \\\n",
    "                                for __ in k] for k in np.load('data_nn.npy',allow_pickle=True).tolist()]\n",
    "                print('NN Load success!')\n",
    "                failed = False\n",
    "        except:\n",
    "                failed = True\n",
    "                print('Load failed. Generating new networks.')\n",
    "else:\n",
    "        load_nn = False\n",
    "        \n",
    "\n",
    "if load_model:\n",
    "        try:\n",
    "                model_params = np.float64(np.load('data_model.npy',allow_pickle=True).tolist()[0])\n",
    "                print('MODEL load success!')\n",
    "                failed = False\n",
    "        except:\n",
    "                failed = True\n",
    "                print('Load failed. 1Generating new networks.')\n",
    "else:\n",
    "        load_model = False\n",
    "\n",
    "if not load_nn:\n",
    "        nn_params_thin         = init_network_params(nn_thin_size, key, nn_scale)\n",
    "        nn_params_outlet     = init_network_params(nn_outlet_size,key,nn_scale)\n",
    "        print('New networks generated.')        \n",
    "if not load_model:\n",
    "        model_params             = init_M_params(model_size, key, model_scale)\n",
    "        print('New model inital guess generated.')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "num_eras = 10000\n",
    "latent_variables = 3\n",
    "\n",
    "frac = .99\n",
    "step = 5\n",
    "data = tuple([_[:int(len(data0[0])*frac)] for _ in data0])\n",
    "samples = np.concatenate((np.arange(0,len(data[0]),step),np.array([len(data[0])-1])))\n",
    "data = tuple([_[samples,:] for _ in data])\n",
    "batch_size = int(len(data[0])*0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def model(model_params,x):\n",
    "        #print('model x: {}'.format(x))\n",
    "        x = np.nan_to_num(x)\n",
    "        k = model_params\n",
    "        return np.array([\n",
    "                                        [-k[0]*x[0]*x[4]+k[1]*x[2]],\n",
    "                                        [k[2]*x[2]*x[3]],\n",
    "                                        [k[0]*x[0]*x[4]-k[1]*x[2]-k[2]*x[2]*x[3]],\n",
    "                                        [-k[2]*x[2]*x[3]],\n",
    "                                        [-k[0]*x[0]*x[4]+k[1]*x[2]+2*k[2]*x[2]*x[3]]\n",
    "                                        ])\n",
    "        \n",
    "        \"\"\"\n",
    "        return np.array([[-mp[0]*(x[0])],0\n",
    "                                         [mp[0]*(x[0]) - mp[1]*((x[1]))],\n",
    "                                         [mp[1]*((x[1]))]])\n",
    "        \"\"\"\n",
    "batched_M = vmap(model,in_axes=(None,0))#, in_axes=(None, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def get_errors(nn_params_thin, nn_params_outlet, model_params, batch):\n",
    "    # need to check come up with outlet integral\n",
    "    x, x_out, t     = batch\n",
    "    eff = 1e-4\n",
    "    eff_model = eff*10.\n",
    "    pred_x_ = batched_state(nn_params_thin, t)\n",
    "    pred_x_out_ = batched_state(nn_params_outlet, t)\n",
    "    err_data    =    (x-pred_x_[:,:-latent_variables])**2\n",
    "    err_data_int    =    (x_out-diff_state(nn_params_outlet,t))**2\n",
    "    pred_x = np.exp(pred_x_)-1.\n",
    "    pred_x_out = np.exp(pred_x_out_)-1. \n",
    "    diff_M = batched_M(model_params,pred_x)[:,:,0]\n",
    "    diff_nn = diff_state(nn_params_thin,t)\n",
    "    err_M_g    = eff_model*((diff_nn[:,:-latent_variables]-diff_M[:,:-latent_variables]))**2\n",
    "    err_M_s    = eff_model*((diff_nn[:,latent_variables-1:])-(diff_M[:,latent_variables-1:]))**2\n",
    "    err_B        =    (pred_x[:,latent_variables-1:].sum(axis=1)-1.)**2\n",
    "    err_MB         =    eff*((pulse_n+(scale_thin*tot_sites*pred_x[0,latent_variables-1:]*mw_s).sum(axis=1)\\\n",
    "                                -(scale_thin*tot_sites*pred_x[1,latent_variables-1:]*mw_s).sum(axis=1)\\\n",
    "                            +(scale_out*pred_x_out[1,:]*mw_g).sum(axis=1))**2).sum()\\\n",
    "                            +eff*batched_state(nn_params_outlet, np.array([[0.]]))**2\n",
    "                            #+((pred_x[0,latent_variables-1:]-bc_s)**2).sum()\n",
    "    barrier = 1e-3\n",
    "    return err_data, err_data_int, err_M_g, err_M_s, err_MB, err_B\\\n",
    "                                 +barrier**2*np.exp((1-model_params[0]/barrier)).mean()\\\n",
    "                                 +barrier**2*np.exp((1.-pred_x/barrier)).mean()\n",
    "                                #+barrier*np.maximum(0.,-model_params).mean()\\\n",
    "                                #+barrier*np.maximum(0.,-pred_x).mean()\\\n",
    "                                #+barrier*np.maximum(0.,-pred_x_out).mean()\\\n",
    "                                #+barrier*np.log(model_params**2).mean()\\\n",
    "                                #+barrier*np.log(pred_x**2).mean()\\\n",
    "                                #+barrier*np.log(pred_x_out**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def loss(nn_params_thin, nn_params_outlet, model_params, batch):\n",
    "    #print('pred[0]: {}'.format(pred_x[0]))\n",
    "    #print('sum pred[0] - bc: {}'.format(sum((pred_x[0]-bc)**2))) \n",
    "    e = get_errors(nn_params_thin, nn_params_outlet, model_params, batch)\n",
    "    return np.array([_.sum() for _ in e]).sum()/(1-np.array([_.var() for _ in e]).var())**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1e-3\n",
    "b1_ = 0.9\n",
    "b2_ = 0.999\n",
    "eps_ = 1e-100\n",
    "opt_init_thin, opt_update_thin, get_params_thin = optimizers.adam(step, b1=b1_, b2=b2_, eps=eps_)#, b1=0.1, b2=0.999, eps=1e-10)\n",
    "opt_init_outlet, opt_update_outlet, get_params_outlet = optimizers.adam(step, b1=b1_, b2=b2_, eps=eps_)\n",
    "opt_init_M, opt_update_M, get_params_M = optimizers.adam(step,b1=b1_, b2=b2_, eps=eps_)\n",
    "    \n",
    "@jit\n",
    "def step(i, opt_state_thin, opt_state_outlet, opt_state_M, batch):\n",
    "    nn_params_thin = get_params_thin(opt_state_thin)\n",
    "    nn_params_outlet = get_params_outlet(opt_state_outlet)\n",
    "    model_params = get_params_M(opt_state_M)\n",
    "    grads_thin = grad(loss,argnums=0)(nn_params_thin, nn_params_outlet, model_params, batch)\n",
    "    grads_outlet = grad(loss,argnums=1)(nn_params_thin, nn_params_outlet, model_params, batch)\n",
    "    grads_M = grad(loss,argnums=2)(nn_params_thin, nn_params_outlet, model_params, batch)\n",
    "    return [opt_update_thin(i, grads_thin, opt_state_thin),\\\n",
    "                    opt_update_outlet(i, grads_outlet, opt_state_outlet),\n",
    "                    opt_update_M(i, grads_M, opt_state_M)]\n",
    "\n",
    "opt_state_thin = opt_init_thin(nn_params_thin)\n",
    "opt_state_outlet = opt_init_outlet(nn_params_outlet)\n",
    "opt_state_M = opt_init_M(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%prun\n",
    "\n",
    "itercount    = itertools.count()        \n",
    "\n",
    "for j in range(num_eras):\n",
    "        #err, _, _, _,_ = get_errors(nn_params_thin, nn_params_outlet, model_params, data)\n",
    "        #err_dist = err.mean(axis=1)#+err_L.mean(axbis=1)#+err_B.mean(axis=1)#+err_M.mean(axis=1)\n",
    "        #err_dist = err_dist/err_dist.sum()     \n",
    "        #sel = np.concatenate((np.array([0,len(data[0])-1]),choice(np.arange(len(data[0])),batch_size,False,err_dist.flatten())))\n",
    "        sel = random.shuffle(key,np.arange(len(data[0])))\n",
    "        sel = np.concatenate((np.array([0,len(data[0])-1]),sel))\n",
    "        batch = tuple([_[sel[:batch_size],:] for _ in data])\n",
    "        #batch = data\n",
    "        for i in range(int(num_epochs)):\n",
    "            it = next(itercount)\n",
    "            opt_state_thin, opt_state_outlet, opt_state_M = step(it, opt_state_thin, opt_state_outlet, opt_state_M, batch)\n",
    "            nn_params_thin = get_params_thin(opt_state_thin)\n",
    "            nn_params_outlet = get_params_outlet(opt_state_outlet)\n",
    "            model_params = get_params_M(opt_state_M)\n",
    "            loss_it_sample = loss(nn_params_thin, nn_params_outlet, model_params, batch)\n",
    "            loss_it_data = loss(nn_params_thin, nn_params_outlet, model_params, data)\n",
    "            #print('Iteration: {:4d}, Loss Batch: {:.7e}, Loss Data: {:.7e}'.format(i,loss_it_sample,loss_it_data))\n",
    "            print('Iteration: {:4d}, Loss Batch: {:.7e}'.format(i,loss_it_sample))\n",
    "            clear_output(wait=True)\n",
    "            #print('--.--')                                \n",
    "nn_params = get_params_nn(opt_state_nn)\n",
    "model_params = get_params_M(opt_state_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#print((batched_state(nn_params,model_params,x)).sum(axis=1))\n",
    "#print((sol.y).sum(axis=0))\n",
    "#def ode2(t,C):\n",
    "#        return model(np.abs(model_params),C).flatten()\n",
    "\n",
    "#sol2 = solve_ivp(ode2, (0, tmax), bc0, t_eval = t_eval)\n",
    "x, x_out, t     = data\n",
    "\n",
    "plt.figure(figsize=[10,10*2./3])\n",
    "plt.plot(t, x)\n",
    "plt.plot(t,(batched_state(nn_params_thin, t)[:,:-latent_variables]),'-o',lw=0.2,ms=2)\n",
    "#plt.plot(sol2.t, sol2.y.T,'-.',lw=0.75,ms=2)\n",
    "plt.legend(labels_g+labels_g)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('C');\n",
    "\n",
    "plt.figure(figsize=[10,10*2./3])\n",
    "plt.plot(t,(batched_state(nn_params_thin, t)[:,latent_variables-1:]),'-o',lw=0.2,ms=2)\n",
    "#plt.plot(sol2.t, sol2.y.T,'-.',lw=0.75,ms=2)\n",
    "plt.legend(labels_s)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('theta');\n",
    "\n",
    "plt.figure(figsize=[10,10*2./3])\n",
    "plt.plot(t, x_out)\n",
    "plt.plot(t,(diff_state(nn_params_outlet, t)),'-o',lw=0.2,ms=2)\n",
    "#plt.plot(sol2.t, sol2.y.T,'-.',lw=0.75,ms=2)\n",
    "plt.legend(labels_g+labels_g)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('C');\n",
    "\n",
    "display(model_params)\n",
    "#display(dict(zip(model_params0,model_params)))\n",
    "#plt.gca().set_xscale('log')\n",
    "#plt.gca().set_yscale('log')\n",
    "\n",
    "err_data, err_data_int, err_M_g, err_M_s, err_MB, err_B = get_errors(nn_params_thin, nn_params_outlet, model_params, batch)\n",
    "print('ERROR\\n')\n",
    "print('Data: {:.3e}'.format(err_data.mean()))\n",
    "print('Data Integral: {:.3e}'.format(err_data_int.mean()))\n",
    "print('M_g: {:.3e}'.format(err_M_g.mean()))\n",
    "print('M_s: {:.3e}'.format(err_M_s.mean()))\n",
    "print('MB: {:.3e}'.format(err_MB.mean()))\n",
    "print('B: {:.3e}'.format(err_B.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data_nn.npy',[nn_params_thin,nn_params_outlet],allow_pickle=True)\n",
    "np.save('data_model.npy',[model_params],allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python3",
   "formats": "ipynb,py:light",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
